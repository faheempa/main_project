{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import environment\n",
    "from collections import deque\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import environment\n",
    "import models\n",
    "import agent\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "esn_input_size = 4\n",
    "esn_reservoir_size = 1024\n",
    "esn_output_size = 128\n",
    "lstm_input_size = 128\n",
    "lstm_num_hidden = 512\n",
    "lstm_num_layers = 4\n",
    "lstm_output_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on test data\n",
    "esn = ESN(esn_input_size, esn_reservoir_size, esn_output_size)\n",
    "esn.load_state_dict(torch.load('trained_models/esn.pt'))\n",
    "lstm = LSTM(lstm_input_size, lstm_num_hidden, lstm_num_layers, lstm_output_size, esn)\n",
    "lstm.load_state_dict(torch.load('trained_models/150_lstm.pt'))\n",
    "agent = Agent(lstm, \"crypto_data/test_data.csv\")\n",
    "\n",
    "state = agent.env.reset()\n",
    "rewards = []\n",
    "while True: \n",
    "    try:\n",
    "        action = agent.get_action(state)\n",
    "        new_state, reward, done = agent.env.step(action)\n",
    "        rewards.append(reward)\n",
    "        agent.remember(state, action, reward)\n",
    "        agent.train_short_memory(state, action, reward)\n",
    "        state = new_state\n",
    "        if done:\n",
    "            agent.train_long_memory()\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total profit:  tensor(42560.6992)\n",
      "Percentage profit:  tensor(91.6368)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total profit: \", agent.env.profit)\n",
    "\n",
    "print(\"Percentage profit: \", agent.env.profit / 46445 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of 1 in rewards:  550\n",
      "No of -1 in rewards:  163\n",
      "No of 0 in rewards:  6\n",
      "Percentage of 1 in rewards:  76.5\n",
      "Percentage of -1 in rewards:  22.7\n",
      "Percentage of 0 in rewards:  0.8\n"
     ]
    }
   ],
   "source": [
    "# no of 1 in rewards\n",
    "print(\"No of 1 in rewards: \", rewards.count(1))\n",
    "print(\"No of -1 in rewards: \", rewards.count(-1))\n",
    "print(\"No of 0 in rewards: \", rewards.count(0))\n",
    "\n",
    "# in percentage\n",
    "print(\"Percentage of 1 in rewards: \", round(rewards.count(1)/len(rewards)*100, 1))\n",
    "print(\"Percentage of -1 in rewards: \", round(rewards.count(-1)/len(rewards)*100, 1))\n",
    "print(\"Percentage of 0 in rewards: \", round(rewards.count(0)/len(rewards)*100, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
