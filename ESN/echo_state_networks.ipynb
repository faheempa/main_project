{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo state networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/echo-state-network-an-overview/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://link.springer.com/chapter/10.1007/11840817_86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define ESN class\n",
    "class ESN(nn.Module):\n",
    "    def __init__(self, input_size, reservoir_size, output_size, spectral_radius=0.9):\n",
    "        super(ESN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Initialize reservoir weights\n",
    "        self.Win = nn.Parameter(torch.randn(reservoir_size, input_size))\n",
    "        self.W = nn.Parameter(torch.randn(reservoir_size, reservoir_size))\n",
    "\n",
    "        # Scaling W to have spectral radius = spectral_radius\n",
    "        self.W.data *= spectral_radius / torch.max(torch.abs(torch.linalg.eigvals(self.W)))\n",
    "\n",
    "        # Output layer\n",
    "        self.Wout = nn.Linear(reservoir_size, output_size)\n",
    "\n",
    "    def forward(self, input_data, initial_state=None):\n",
    "        if initial_state is None:\n",
    "            state = torch.zeros((input_data.size(0), self.reservoir_size)).to(input_data.device)\n",
    "        else:\n",
    "            state = initial_state\n",
    "\n",
    "        state = torch.tanh(torch.matmul(input_data, self.Win.t()) + torch.matmul(state, self.W.t()))\n",
    "\n",
    "        output = self.Wout(state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:05<00:00, 1787697.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 1082975.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 3433765.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate ESN\n",
    "input_size = 28 * 28  # MNIST image size\n",
    "reservoir_size = 1000  # Size of reservoir\n",
    "output_size = 10  # Number of classes\n",
    "esn = ESN(input_size, reservoir_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(esn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.796\n",
      "[1,   200] loss: 0.404\n",
      "[1,   300] loss: 0.351\n",
      "[1,   400] loss: 0.303\n",
      "[1,   500] loss: 0.281\n",
      "[1,   600] loss: 0.275\n",
      "[1,   700] loss: 0.272\n",
      "[1,   800] loss: 0.257\n",
      "[1,   900] loss: 0.247\n",
      "[2,   100] loss: 0.207\n",
      "[2,   200] loss: 0.221\n",
      "[2,   300] loss: 0.212\n",
      "[2,   400] loss: 0.221\n",
      "[2,   500] loss: 0.203\n",
      "[2,   600] loss: 0.211\n",
      "[2,   700] loss: 0.206\n",
      "[2,   800] loss: 0.193\n",
      "[2,   900] loss: 0.200\n",
      "[3,   100] loss: 0.179\n",
      "[3,   200] loss: 0.164\n",
      "[3,   300] loss: 0.183\n",
      "[3,   400] loss: 0.172\n",
      "[3,   500] loss: 0.180\n",
      "[3,   600] loss: 0.168\n",
      "[3,   700] loss: 0.168\n",
      "[3,   800] loss: 0.187\n",
      "[3,   900] loss: 0.169\n",
      "[4,   100] loss: 0.140\n",
      "[4,   200] loss: 0.155\n",
      "[4,   300] loss: 0.135\n",
      "[4,   400] loss: 0.145\n",
      "[4,   500] loss: 0.151\n",
      "[4,   600] loss: 0.162\n",
      "[4,   700] loss: 0.141\n",
      "[4,   800] loss: 0.149\n",
      "[4,   900] loss: 0.147\n",
      "[5,   100] loss: 0.118\n",
      "[5,   200] loss: 0.137\n",
      "[5,   300] loss: 0.122\n",
      "[5,   400] loss: 0.128\n",
      "[5,   500] loss: 0.132\n",
      "[5,   600] loss: 0.125\n",
      "[5,   700] loss: 0.132\n",
      "[5,   800] loss: 0.137\n",
      "[5,   900] loss: 0.137\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 28 * 28)  # Flatten images\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = esn(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the network\n",
    "testset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 28 * 28)\n",
    "        outputs = esn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 94 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the network on the 10000 test images: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down how the Echo State Network (ESN) works line by line:\n",
    "\n",
    "1. **Class Definition**: \n",
    "    - `class ESN(nn.Module):`: This line defines a Python class named `ESN`, which is a subclass of `nn.Module` from PyTorch. This means that `ESN` is a neural network module that can be trained using PyTorch's functionalities.\n",
    "\n",
    "2. **Initialization Method (`__init__`)**:\n",
    "    - `def __init__(self, input_size, reservoir_size, output_size, spectral_radius=0.9):`: This is the initialization method of the class `ESN`. It takes input parameters such as `input_size`, `reservoir_size`, `output_size`, and `spectral_radius` (with a default value of 0.9). These parameters define the architecture and properties of the ESN.\n",
    "\n",
    "3. **Initialization of Parameters**:\n",
    "    - `super(ESN, self).__init__()`: This line calls the constructor of the superclass `nn.Module`.\n",
    "    - `self.input_size = input_size`: Stores the input size for later use.\n",
    "    - `self.reservoir_size = reservoir_size`: Stores the reservoir size (number of reservoir neurons).\n",
    "    - `self.Win = nn.Parameter(torch.randn(reservoir_size, input_size))`: Initializes the input weights (`Win`) as trainable parameters using `nn.Parameter`. These weights are randomly initialized from a Gaussian distribution.\n",
    "    - `self.W = nn.Parameter(torch.randn(reservoir_size, reservoir_size))`: Initializes the reservoir weights (`W`) in a similar manner.\n",
    "    \n",
    "4. **Scaling the Reservoir Weights**:\n",
    "    - `self.W.data *= spectral_radius / torch.max(torch.abs(torch.linalg.eigvals(self.W)))`: This line scales the reservoir weights (`W`) to ensure the spectral radius (the maximum absolute eigenvalue) is equal to the `spectral_radius` parameter. This is important for stability and proper functioning of the network.\n",
    "\n",
    "5. **Output Layer**:\n",
    "    - `self.Wout = nn.Linear(reservoir_size, output_size)`: Defines a linear transformation (`nn.Linear`) to map the reservoir state to the output space. This will be used for prediction.\n",
    "\n",
    "6. **Forward Method**:\n",
    "    - `def forward(self, input_data, initial_state=None):`: Defines the forward pass method of the ESN, which computes the output of the network given an input.\n",
    "    - `if initial_state is None:`: Checks if an initial state is provided. If not, initializes the state to zeros.\n",
    "    - `else:`: Handles the case where an initial state is provided.\n",
    "    - `state = torch.tanh(torch.matmul(input_data, self.Win.t()) + torch.matmul(state, self.W.t()))`: This line computes the new state of the reservoir neurons using the input data, the input weights (`Win`), the current state, and the reservoir weights (`W`). It applies the hyperbolic tangent activation function (`torch.tanh`) to the sum of the input and recurrent activations.\n",
    "    - `output = self.Wout(state)`: Computes the output by passing the reservoir state through the output layer (`Wout`), which is a linear transformation.\n",
    "\n",
    "This breakdown provides an overview of how an Echo State Network (ESN) is implemented and how it processes input data to produce an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the reservoir weights \n",
    "Crucial for ensuring stability and control over the network's dynamics. Here's why we need to scale the reservoir weights:\n",
    "\n",
    "1. **Spectral Radius Control**: The spectral radius of the reservoir weight matrix has a significant impact on the dynamics of the network. It determines how information propagates through the network during the recurrent computation. If the spectral radius is too large, the network might become unstable and exhibit chaotic behavior. If it's too small, the network might not capture enough information from the input.\n",
    "\n",
    "2. **Echo State Property**: The Echo State Property (ESP) is a key characteristic of ESNs. It states that the effect of the initial state on the network's output diminishes over time, and only the input history matters. Scaling the reservoir weights helps maintain this property by controlling the magnitude of the recurrent activations.\n",
    "\n",
    "3. **Avoiding Saturation**: Large reservoir weights can cause activations to saturate, leading to vanishing or exploding gradients during training. Scaling the weights helps prevent this issue, ensuring that the network can learn effectively.\n",
    "\n",
    "4. **Stability**: Scaling the weights to have a spectral radius within a desired range ensures that the network operates within stable regions of its activation functions, preventing runaway activations that can lead to numerical instability.\n",
    "\n",
    "Overall, scaling the reservoir weights is essential for controlling the dynamics of an ESN, ensuring stability during training and effective information processing. It helps maintain the network's Echo State Property and facilitates learning of meaningful representations from the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The calculation of the state in the forward method\n",
    "\n",
    "```python\n",
    "state = torch.tanh(torch.matmul(input_data, self.Win.t()) + torch.matmul(state, self.W.t()))\n",
    "```\n",
    "\n",
    "1. **Input Data**: `input_data` represents the input to the ESN. In the context of image classification, `input_data` typically consists of flattened images, where each row corresponds to a single image.\n",
    "\n",
    "2. **Input Weights (Win)**: `self.Win` represents the input weights of the ESN. It's a parameter of the network that defines the connections from the input to the reservoir neurons. The transpose (`self.Win.t()`) is taken because the input data is multiplied by the transpose of the input weights to match the dimensions.\n",
    "\n",
    "3. **Reservoir Weights (W)**: `self.W` represents the reservoir weights of the ESN. These weights define the recurrent connections between reservoir neurons. Similar to the input weights, the transpose (`self.W.t()`) is taken because the current state is multiplied by the transpose of the reservoir weights to match the dimensions.\n",
    "\n",
    "4. **Activation Function (Tanh)**: The hyperbolic tangent function (`torch.tanh`) is applied element-wise to the sum of the input activations (`torch.matmul(input_data, self.Win.t())`) and the recurrent activations (`torch.matmul(state, self.W.t())`). This sum represents the total input to each reservoir neuron.\n",
    "\n",
    "5. **State Update**: The result of applying the hyperbolic tangent function is the new state of the reservoir neurons. This updated state (`state`) is computed for each sample in the input batch. It represents the activation levels of the reservoir neurons after processing the input data.\n",
    "\n",
    "In summary, the calculation of the state in the forward method involves computing the total input to each reservoir neuron by combining the input data with the current state through the input and reservoir weights. This total input is then passed through the hyperbolic tangent activation function to produce the updated state of the reservoir neurons. This process is performed for each sample in the input batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The recurrent connections and the Echo State Property\n",
    "\n",
    "fundamental aspects of the network's architecture and behavior. Let's break down where these occur and how the Echo State Property is established:\n",
    "\n",
    "1. **Recurrent Connections**:\n",
    "   - Recurrent connections are established through the reservoir weights (`self.W`). These weights define the connections between reservoir neurons, allowing information to propagate and persist over time within the network.\n",
    "   - In the forward method of the ESN (`forward(self, input_data, initial_state=None)`), the recurrent connections are applied when computing the new state of the reservoir neurons:\n",
    "     ```python\n",
    "     state = torch.tanh(torch.matmul(input_data, self.Win.t()) + torch.matmul(state, self.W.t()))\n",
    "     ```\n",
    "   - Here, `torch.matmul(state, self.W.t())` represents the recurrent connections. It computes the contribution of the current state to the next state by multiplying the current state (`state`) by the transpose of the reservoir weights (`self.W.t()`).\n",
    "\n",
    "2. **Echo State Property**:\n",
    "   - The Echo State Property (ESP) is a key characteristic of ESNs, ensuring that the effect of the initial state on the network's output diminishes over time, and only the input history matters.\n",
    "   - The ESP is achieved through the combination of random initialization of the reservoir neurons, fixed recurrent connections, and input-driven training.\n",
    "   - The random initialization of the reservoir neurons ensures that each neuron has a diverse range of activation levels, contributing to the network's rich dynamics.\n",
    "   - The fixed recurrent connections, established by the reservoir weights (`self.W`), create a dynamic reservoir that retains and processes information over time.\n",
    "   - During training, the network learns to map input patterns to desired outputs using a linear readout layer (`self.Wout`), while the recurrent dynamics of the reservoir provide rich temporal representations of the input history.\n",
    "   - The ESP ensures that the network's internal dynamics effectively encode temporal information from the input sequences, facilitating accurate prediction or classification tasks.\n",
    "\n",
    "In summary, recurrent connections are established through the reservoir weights, and the Echo State Property is manifested through the dynamic interplay of random initialization, fixed recurrent connections, and input-driven training, allowing the network to effectively process temporal information while exhibiting stable and rich dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features extracted by an Echo State Network (ESN)\n",
    "\n",
    "When using an Echo State Network (ESN) as a feature extractor, the features extracted from the input data are representations learned by the reservoir neurons based on the input patterns. These features are not explicitly defined by humans but are learned by the network during training. Let's discuss the nature of these features and how they can be interpreted by humans and neural networks:\n",
    "\n",
    "1. **Nature of Features**:\n",
    "   - The features extracted by the ESN are abstract representations of the input data that capture relevant information for the task at hand (e.g., classification, prediction).\n",
    "   - Since the reservoir neurons have nonlinear activation functions and recurrent connections, the features learned by the ESN can be complex and nonlinear transformations of the input data.\n",
    "   - These features are typically distributed representations, meaning that each feature (neuron activation) may encode information from multiple input dimensions.\n",
    "   - The features are learned in an unsupervised manner, meaning that the network autonomously discovers patterns and structures in the input data without explicit labeling or supervision.\n",
    "\n",
    "2. **Interpretation by Humans**:\n",
    "   - Humans may find it challenging to interpret the features learned by the ESN directly, especially in high-dimensional spaces.\n",
    "   - However, visualization techniques such as dimensionality reduction (e.g., t-SNE, PCA) can be used to project the high-dimensional feature space into a lower-dimensional space for visualization and interpretation.\n",
    "   - Interpretation of the features often relies on understanding which input patterns or characteristics are encoded by specific features. This can be inferred by analyzing the patterns of activation across different input samples.\n",
    "\n",
    "3. **Interpretation by Neural Networks**:\n",
    "   - Subsequent neural networks (e.g., feedforward neural networks, support vector machines) can interpret the features extracted by the ESN for downstream tasks such as classification or regression.\n",
    "   - These neural networks treat the features extracted by the ESN as input features and learn to map them to the target outputs through supervised learning.\n",
    "   - The neural networks can learn complex decision boundaries or relationships between the extracted features and the target outputs, leveraging the representational power of the ESN features for improved performance on the task.\n",
    "\n",
    "In summary, when using an ESN as a feature extractor, the features extracted are abstract representations of the input data learned by the network. While these features may be difficult for humans to interpret directly, they can be effectively utilized by subsequent neural networks for various tasks, leading to improved performance compared to using raw input data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
